{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPJ4GhLjkc+XvAyOcep9hHp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"z9evlzF4t7K0"},"outputs":[],"source":["!pip install -U transformers datasets seqeval -q"]},{"cell_type":"code","source":["import numpy as np\n","from datasets import load_dataset  # load dataset from Hugging Face\n","from transformers import AutoTokenizer, AutoModelForTokenClassification  # load correct tokenizer for model and BERT for NER\n","from transformers import TrainingArguments, Trainer  # TrainingArguments: configure training , Trainer: handles training loop\n","from seqeval.metrics import classification_report # evaluate NER performance"],"metadata":{"id":"kMB36BLTuEnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset(\"lhoestq/conll2003\")  # load CoNLL-2003, it contains tokens, ner_tag(label)\n","print(dataset)"],"metadata":{"id":"oVnENW6nvVs6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # bert-base-uncased tokenizer splits words into subwords and adds special tokens [CLS] & [SEP]"],"metadata":{"id":"i-MA5Kd6vIVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset[\"train\"].features)"],"metadata":{"id":"LGoteL4dioz9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_list = [\n","    \"O\",\n","    \"B-PER\", \"I-PER\",\n","    \"B-ORG\", \"I-ORG\",\n","    \"B-LOC\", \"I-LOC\",\n","    \"B-MISC\", \"I-MISC\"\n","]\n","\n","# Tokenize and align labels\n","# original : [\"California\"] labels-> [B-LOC] , BERT split into: [\"cal\", \"##ifornia\"] so need to align labels correctly\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(\n","        examples[\"tokens\"],\n","        truncation=True,  # cut off any input text that exceeds the max length\n","        is_split_into_words=True # input is already tokenized\n","    )\n","\n","    all_labels = []\n","\n","    # align lables\n","    for i, labels in enumerate(examples[\"ner_tags\"]):\n","      # examples[\"ner_tags\"] → list of label sequences (one per sentence)\n","      # labels → the label list for one sentence\n","      # index of the sentence in the batch\n","\n","        word_ids = tokenized_inputs.word_ids(batch_index=i) # tells which token belongs to which word\n","        # EX:\n","        ## Original words: [\"John\", \"Washington\"]\n","        ## Tokenized: [\"[CLS]\", \"john\", \"wash\", \"##ington\", \"[SEP]\"]\n","        ## word_ids becomes: [None, 0, 1, 1, None]\n","\n","        previous_word_idx = None # used to detect subwords\n","        label_ids = [] # final aligned labels for tokens\n","\n","        for word_idx in word_ids:\n","            if word_idx is None: # special tokens : CLS, SEP\n","                label_ids.append(-100) # -100: PyTorch loss function ignores -100, so model doesn't learn from special tokens\n","            elif word_idx != previous_word_idx: # at the first token of a word\n","                label_ids.append(labels[word_idx]) # assign the real NER label\n","            else: # word_idx == previous_word_idx : token is a continuation of the same word\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","\n","        all_labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = all_labels\n","    return tokenized_inputs\n","\n","\n","tokenized_datasets = dataset.map(\n","    tokenize_and_align_labels,\n","    batched=True,  # process in batches\n","    num_proc=4, # use 4 CPU cores\n","    remove_columns=dataset[\"train\"].column_names  # remove original columns\n",")"],"metadata":{"id":"_MjzecLVJT-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForTokenClassification.from_pretrained(\n","    \"bert-base-uncased\",\n","    num_labels=len(label_list)\n",")"],"metadata":{"id":"h67hikC6JW0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"./ner_results\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=1,\n","    weight_decay=0.01, # regularization\n","    logging_dir=\"./logs\",\n",")"],"metadata":{"id":"wuuxodvOKmeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2) # Choose highest probability label for each token\n","\n","    true_labels = [\n","        [label_list[l] for l in label if l != -100]  # if label is NOT -100, Convert integer ID in to label name\n","        for label in labels  # loop through each sentence\n","    ]  # EX: [0, 0, 1, 2, -100, -100] -> [\"O\", \"O\", \"B-PER\", \"I-PER\"]\n","\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(pred, label) if l != -100]  # loop token by token and only keep predictions for real tokens\n","        for pred, label in zip(predictions, labels)  # loop through predicted sentence and true sentence\n","    ]\n","\n","    print(classification_report(true_labels, true_predictions))\n","    return {}"],"metadata":{"id":"gkX4qdlmKrzf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForTokenClassification\n","\n","# data collator: dynamically pads inputs, Pads labels correctly, keeps alignment between tokens and labels\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","trainer = Trainer(  # high-level training engine from Transformers\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)),\n","    eval_dataset=tokenized_datasets[\"validation\"].select(range(500)),\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"WpaBc5zLKuj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()  # load batch, pad using data_collator, forward pass, compute loss, backpropagation, update weights, repeat"],"metadata":{"id":"ACIr0iW7Kw77"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"Yj5C5L3qLI0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # check GPU is available then use cude otherwise CPU\n","model.to(device) # move model to device , if model on GPU but input is on CPU -> error\n","\n","text = \"John works at Google in California\"\n","tokens = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)  # return_tensors=\"pt\" : return PyTorch tensors , # .to(device): move tokenized tensors to GPU\n","\n","with torch.no_grad(): # not training so do not compute gradients\n","    outputs = model(**tokens) # run the model\n","\n","predictions = outputs.logits.argmax(dim=2) # selects label with highest probability\n","\n","predicted_labels = [label_list[p.item()] for p in predictions[0]]  # convert label IDs into label names\n","\n","for token, label in zip(tokenizer.tokenize(text), predicted_labels[1:-1]):  # print token and label\n","    print(f\"{token}: {label}\")"],"metadata":{"id":"Y74Gr59SLXs0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mks8g2--CBE_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_2qROoAOCKfn"},"execution_count":null,"outputs":[]}]}